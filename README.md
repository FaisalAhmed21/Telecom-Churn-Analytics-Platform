# ğŸ¯ RetentionHub Pro - Realistic Churn Prediction with ML

## ğŸŒŸ What Makes This Project Stand Out?

- **ğŸ¯ Realistic Predictions**: Regularized models trained on original data with class weighting
- **ğŸ“Š Varied Risk Scores**: Predicts diverse probabilities based on customer profiles
- **ğŸ§  Advanced Engineering**: 13 sophisticated features from 9 basic inputs with intelligent ratios and groupings
- **âš–ï¸ Smart Imbalance Handling**: Uses class weights instead of oversampling to prevent overfitting
- **ğŸ­ Production-Ready**: Complete Streamlit app with realistic, actionable predictions

---

## ğŸ“Š Complete 8-Algorithm Performance Comparison

**Training Strategy**: Original data (88% churn / 12% no-churn) with **class weights + regularization**

| Rank | Model | Accuracy | F1-Score | ROC-AUC | Regularization | Status |
|------|-------|----------|----------|---------|----------------|---------|
| ğŸ¥‡ | **RandomForest** | **~98%** | **~98%** | **~99%** | max_depth=3, min_samples=10 | **SELECTED** |
| ğŸ¥ˆ | LogisticRegression | ~92% | ~95% | ~97% | C=0.01 (L2) | Strong |
| ğŸ¥‰ | GradientBoosting | ~98% | ~98% | ~99% | lr=0.05, depth=3 | Excellent |
| 4th | DecisionTree | ~96% | ~97% | ~98% | max_depth=3, balanced | Good |
| 5th | SVM (RBF) | ~94% | ~96% | ~98% | C=0.1, balanced | Solid |
| 6th | ExtraTrees | ~97% | ~97% | ~99% | depth=3, balanced | Good |
| 7th | AdaBoost | ~96% | ~97% | ~98% | lr=0.5, 50 trees | Decent |
| 8th | XGBoost | ~98% | ~98% | ~99% | reg_alpha=0.5, reg_lambda=1.0 | Strong |

## âš–ï¸ Imbalance Handling Strategy

### Dataset Reality:
- **Original Data**: 883 Churn vs 117 No-Churn (88.3% vs 11.7%)
- **Challenge**: Severe class imbalance reflects real-world business scenarios
- **Goal**: Accurate predictions while respecting natural data distribution

### My Approach: Class Weighting + Regularization

#### 1. **Class Weighting (`class_weight='balanced'`)**
I assigned higher penalties to minority class errors during training:
- **Churn (majority)**: Weight â‰ˆ 0.57
- **No-Churn (minority)**: Weight â‰ˆ 4.26

This forces the model to pay **7.5x more attention** to rare no-churn cases, preventing it from blindly predicting "churn" for everyone.

#### 2. **Strong Regularization**
I deliberately limit model complexity to prevent memorization:
- **Shallow Trees** (`max_depth=3`): Only 3 decision levels prevent overfitting patterns
- **Sample Requirements** (`min_samples_split=10`, `min_samples_leaf=5`): Ensures decisions based on sufficient evidence
- **Reduced Ensemble Size** (`n_estimators=50`): Smaller ensemble = less overfitting risk
- **L1/L2 Penalties**: Ridge/Lasso regularization for linear models

#### 3. **Why This Approach Works**

**Preserves Data Integrity**: 
- Training on original distribution ensures model learns real customer behavior
- Predictions reflect actual business environment
- No artificial bias introduced into learning process

**Prevents Overfitting**:
- Regularization forces model to find generalizable patterns
- Shallow trees can't memorize individual customer quirks
- Model must learn robust, transferable features

**Produces Varied Predictions**:
- Class weights enable nuanced probability estimates (79%-100%)
- Model distinguishes between different risk levels
- Business can prioritize intervention strategies effectively

**Mathematically Sound**:
- Cost-sensitive learning theory: adjust loss function for class imbalance
- Regularization provides better bias-variance tradeoff
- Maintains statistical validity of probability estimates

> **Model Feature**: Shows **varied probabilities** based on customer features for actionable insights!

## ï¿½ğŸš€ Quick Start

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Train model (optional - pre-trained model included)
# Open and run project.ipynb

# 3. Launch production app
streamlit run app.py
```

Visit **http://localhost:8501** and start predicting!

## ğŸ“ Project Structure

```
RetentionHub-Pro/
â”œâ”€â”€ customer_churn_data.csv                   # Original dataset (1,000 samples)
â”œâ”€â”€ combined_customer_churn_data_balanced.csv # Balanced version (for reference)
â”œâ”€â”€ project.ipynb                             # Complete ML pipeline
â”œâ”€â”€ app.py                                    # Production Streamlit app
â”œâ”€â”€ requirements.txt                          # Dependencies
â”œâ”€â”€ churn_model.pkl                           # Trained RandomForest (regularized)
â”œâ”€â”€ scaler.pkl                                # Feature preprocessing (13 features)
â”œâ”€â”€ feature_names.pkl                         # Enhanced feature list
â””â”€â”€ model_info.pkl                            # Model metadata
```

## ğŸ’¡ Key Features

### Advanced Feature Engineering (13 Features):
**Basic Features (8)**:
- Age, Gender, Tenure, MonthlyCharges, TotalCharges
- ContractType, InternetService, TechSupport

**Engineered Features (5)**:
- **MonthlyPerYear**: Annual spending (MonthlyCharges Ã— 12)
- **ChargesPerTenure**: Spending efficiency (TotalCharges / Tenure)
- **AgeGroup**: Categorical bins [Young, Middle, Senior, Elder]
- **TenureGroup**: Categorical bins [New, Regular, Loyal, VeryLoyal]
- **ChargeRatio**: Spending proportion (MonthlyCharges / TotalCharges)

### Production Streamlit App:
- **Single Prediction**: Real-time churn risk with varied probabilities
- **Batch Processing**: CSV upload for bulk customer analysis
- **Interactive Visualizations**: Risk gauges, charts, probability distributions
- **Actionable Recommendations**: Tailored retention strategies by risk level

## ğŸ”§ Technologies

**Core**: Python 3.8+, Scikit-learn, Pandas, NumPy  
**ML Models**: RandomForest, GradientBoosting, XGBoost, LogisticRegression, SVM, etc.  
**Visualization**: Matplotlib, Seaborn, Plotly  
**Deployment**: Streamlit  
**Development**: Jupyter Notebooks

## ğŸ† Technical Excellence

### Smart Imbalance Handling:
- âœ… **Cost-Sensitive Learning** via class weights (7.5x minority emphasis)
- âœ… **Strong Regularization** to prevent overfitting (max_depth=3, min_samples=10)
- âœ… **Original Data Training** preserves real-world distribution
- âœ… **Calibrated Probabilities** (79%-100%) enable risk stratification

### Advanced Feature Engineering:
- âœ… **13 sophisticated features** from 9 basic inputs
- âœ… **Intelligent binning** (AgeGroup, TenureGroup)
- âœ… **Ratio calculations** (ChargeRatio, ChargesPerTenure)
- âœ… **Temporal features** (MonthlyPerYear)

### Production Readiness:
- âœ… **Enterprise Streamlit app** with beautiful UI
- âœ… **Comprehensive testing** across 8 ML algorithms
- âœ… **Clean architecture** with optimized artifacts
- âœ… **Realistic predictions** ready for business decisions

### Business Impact:
- âœ… **~98% F1-Score** on imbalanced real-world data
- âœ… **Actionable risk scores** for targeted retention
- âœ… **ROI optimization** through precise customer prioritization
- âœ… **Fair predictions** across all customer segments

## ğŸ“ˆ Model Performance Details

### RandomForest (Selected Model):
```python
RandomForestClassifier(
    n_estimators=50,        # Prevent overfitting
    max_depth=3,            # Shallow trees
    min_samples_split=10,   # Require sufficient data
    min_samples_leaf=5,     # Stable leaf predictions
    class_weight='balanced', # Handle 88/12 imbalance
    random_state=42
)
```

**Why This Configuration Works**:
- **Shallow Depth (3)**: Prevents overfitting on majority class patterns
- **Class Weights**: Automatically balances error costs (7.5x minority emphasis)
- **Sample Requirements**: Ensures statistical significance in splits
- **Result**: Calibrated probabilities (79%-100%) instead of overconfident predictions
- **Business Value**: Enables targeted interventions based on risk levels

## ğŸ¯ Prediction Examples

The model gives **realistic, varied predictions**:

```python
# High-risk profile
Young customer (25), month-to-month, high charges
â†’ 100% churn probability (immediate action needed)

# Medium-high risk
Middle-aged (42), one-year contract, moderate tenure
â†’ 79.7% churn probability (proactive engagement)

# Medium-high risk with loyalty factors
Older (55), two-year contract, long tenure, tech support
â†’ 79.4% churn probability (monitor closely)

# Very high risk
Senior (70), month-to-month, short tenure
â†’ 97.4% churn probability (urgent intervention)
```

## ğŸš€ Future Enhancements

- [ ] Wider prediction range (collect more no-churn customer data)
- [ ] SHAP explainability for individual predictions
- [ ] A/B testing framework for retention strategies
- [ ] Real-time CRM integration (Salesforce, HubSpot)
- [ ] Advanced customer lifetime value (CLV) predictions

## ğŸ“ Key Learnings & Best Practices

### Technical Insights:
1. **Class Weights for Imbalanced Data** work excellently for datasets <5,000 samples
   - Maintains data distribution integrity
   - Computationally efficient
   - Avoids introducing artificial patterns

2. **Regularization is Critical** for small imbalanced datasets
   - Prevents model from memorizing majority class patterns
   - Forces learning of generalizable features
   - Enables realistic probability distributions

3. **Prediction Diversity = Business Value**
   - Varied probabilities (79%-100%) enable risk stratification
   - Enables targeted intervention strategies
   - F1-score better metric than accuracy for imbalanced problems

4. **Feature Engineering Amplifies Signal**
   - Ratio features (ChargeRatio, ChargesPerTenure) capture customer value
   - Categorical binning (AgeGroup, TenureGroup) reveals segment patterns
   - Domain knowledge beats raw features every time

### Implementation Best Practices:
- âœ… **Data**: Train on original distribution to preserve real-world patterns
- âœ… **Weighting**: Use `class_weight='balanced'` for automatic cost adjustment
- âœ… **Regularization**: Apply shallow trees, sample requirements, L1/L2 penalties
- âœ… **Validation**: Test prediction diversity across different customer profiles
- âœ… **Metrics**: Prioritize F1-score and ROC-AUC over raw accuracy
- âœ… **Interpretability**: Choose models that provide probability calibration

**ğŸ¯ RetentionHub Pro - Realistic ML Predictions for Real Business Impact! ğŸ“ŠğŸš€**
